
from pyspark.sql import SparkSession
from pyspark.sql import DataFrame
from pyspark.sql.functions import col,row_number,lit
from pyspark.sql.window import Window
from pyspark.sql.functions import *
import os
import yaml
import re
from lib.dataprep  import Dataprep
from typing import Any, Dict
from parameters import DEFAULT_PARAMETERS

def getModelTemplate(model_dict:Dict[str,Any]) -> Dataprep:
    #print(model_dict)
    return Dataprep(
                    data_parameters         = model_dict['data']
                    ,row_key_split          = model_dict['row_key_compose']
                    ,features_column        = model_dict['features']
                    ,targets_columns        = model_dict['targets']
                    ,target_filters         = model_dict['filters']['target_level']
                    ,features_filters      = model_dict['filters']['feature_level']
                    ,query_moment_conf      = model_dict['query_moments']['fixed']
                    ,query_moment_reference = model_dict['query_moments']['reference']
                  ) 

def read_config_file(file_path:str)->Dataprep:
    """.
    :param file_path: Config file.
    :return: Python dict with all config data file.
    """
    with open(file_path) as file:
         return getModelTemplate(yaml.load(file,Loader=yaml.FullLoader))

# def read_config_file(file_path:str)->dict:
#     """Returns a dataset with filters applied in config.yaml file.
#     :param file_path: Config file.
#     :return: Python dict with all config data file.
#     """
#     with open(file_path) as file:
#         config_dict = yaml.load(file,Loader=yaml.FullLoader)
#     return config_dict
    
def read_parquet_file(spark_session:SparkSession,
                      parquet_read_folder:str,
                      date_window_start:str,
                      date_window_end:str) ->DataFrame:
    """
    Process the read raw data (generated by ETL Process) and save the preprocessed dataset

    :param spark: SparkSession to run the script
    :param parquet_read_folder: str with parquet file path
    :param date_window_start: str with start partition_date ex: 2020-01-01
    :param date_window_end: str with end partition_date ex: 2020-01-02
    :return: DataFrame
    """                  
    return spark_session.read\
                            .parquet(parquet_read_folder)\
                            .filter(col('partition_data').between(date_window_start,date_window_end))


def save_file(output_dataframe:DataFrame,dataprep:Dataprep,spark_session:SparkSession,file_path:str) ->bool:
    """
    Process the save dataframe processed in parquet file
    :param output_dataframe: Dataframe processed.
    :param config_file: yaml file with all attribures applied in Config.yaml
    :param spark: SparkSession to run the script
    :return: Bool
    Note:In case to run in pks-cluster, the default folder will be /opt/data/processed
    """     
    data_parameters = dataprep.data
    if (data_parameters.get('mode')=='cluster'):
        #print("In schema cluster")
        save_file_in_hadoop_cluster(output_dataframe=output_dataframe,data_parameters=data_parameters,spark_session=spark_session)
    else:
        print("In schema out")
        save_file_in_external_cluster(output_dataframe=output_dataframe,
                                      dataprep=dataprep,
                                      file_path=file_path)



def save_file_in_hadoop_cluster(output_dataframe:DataFrame,data_parameters:dict,spark_session:SparkSession) ->bool:
    w = Window.partitionBy('row_key:full','query_moment_value').orderBy(col("version").desc())
    if(check_if_file_exists(data_parameters,spark_session)):
        output_dataframe = output_dataframe.withColumn('version',lit('1'))
        actual_dataframe = spark_session.read.parquet(f"{data_parameters.get('output_folder')}"
                                                      f"/{data_parameters.get('project_name')}/"
                                                      f"{data_parameters.get('version')}").cache()
        actual_dataframe.count()                                              
        actual_dataframe = actual_dataframe.withColumn('version',lit('2'))
        output_dataframe = output_dataframe.union(actual_dataframe)\
                                                                .withColumn('rn',row_number().over(w))\
                                                                .filter('rn == 1')\
                                                                .drop('rn')\
                                                                .drop('version')
        #output_dataframe.show(truncate=False,n=1,vertical=True)
    output_dataframe.write.mode('overwrite').parquet(f"{data_parameters.get('output_folder')}/"
                                                     f"{data_parameters.get('project_name')}/"
                                                      f"{data_parameters.get('version')}"
                                                         )

def save_file_in_external_cluster(output_dataframe:DataFrame,dataprep:Dataprep,file_path:str) ->bool:
    data_info = dataprep.data
    print(data_info)
    path_file = f"{data_info.get('output_folder')}/{data_info.get('project_name')}/{data_info.get('version')}"
    output_dataframe.write \
                        .mode('overwrite')\
                        .parquet(f"{path_file}/data")
    output_dataframe.count()
    os.system(f"hadoop fs -put -f {file_path} {path_file}/dataprep_{data_info.get('version')}_config.yaml")

def check_if_file_exists(data_parameters:dict,spark_session:SparkSession)->bool:
    fs = spark_session._jvm.org.apache.hadoop.fs.FileSystem.get(spark_session._jsc.hadoopConfiguration())
    return fs.exists(spark_session._jvm.org.apache.hadoop.fs.Path(f"{data_parameters.get('output_folder')}/"
                                                f"{data_parameters.get('project_name')}/"))


def validade_path_project_name(project_name:str)->bool:
    """Confirm if folder name existis in project """
    print(project_name)
    print(os.listdir('.'))
    if (project_name in [f for f in os.listdir('.') if re.match(r'prep_', f)]):
        return f'{project_name}/config.yml'
    else : 
         raise SystemExit('Error ---** Please confirm the project Name parameter **---')


         
def validade_config_file_parameters(
                                    path_project:str,
                                    dataprep:Dataprep
                                    )->bool:
                                    
    DEFAULT_HDFS_FOLDER = DEFAULT_PARAMETERS.get('HDFS_FOLDER')
    DEFAULT_PATH_PROJECT= DEFAULT_PARAMETERS.get('PATH_PROJECT')
    #print(DEFAULT_PARAMETERS.get('HDFS_FOLDER')) 
    #print(DEFAULT_PARAMETERS.get('PATH_PROJECT')) 
    #print(path_project)
    #print(dataprep.data.get('output_folder'))
    """Confirm if folder name existis in project """
    if dataprep.data.get('mode')=='cluster'  and path_project==DEFAULT_PATH_PROJECT and dataprep.data.get('output_folder')==DEFAULT_HDFS_FOLDER:
        return True
    elif dataprep.data.get('mode')!='cluster' and dataprep.data.get('output_folder')!=DEFAULT_HDFS_FOLDER and path_project!=DEFAULT_PATH_PROJECT:
        return True

    else : 
        raise SystemExit('Error ---** Please confirm the config.yml parameters **---')